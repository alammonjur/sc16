\section{Introduction}
A priority queue is a popular data structure used for various applications such as routing, anomaly prioritization, shortest path search, and scheduling~\cite{ah1,ah2,ah3}.
It is a data structure in which (1) each element has a priority, and (2) a dequeue operation removes and returns the highest priority element from the queue.
%The concept is a basic component for scheduling used in most routers and event-driven simulators \cite{hw1,fpga1}.

The concurrent priority queue proposed by Vipin {\it et. al}~\cite{pq6} leads a new direction of parallel heap processing. 
It shows how insert and delete operations can be processed from the root node; in particular, the authors formulate the insertion path from the root to the last node. 
Although the paper does not show fine-grained cycle operation during the concurrent process, it theoretically shows the complexity of concurrent heap processing by addressing the locking meachnism at system-level implementation.

Moving forward from the concurrent software priority queue, there are several parallel hardware implementations of a priority queue~\cite{hw1,hw2,hw8,hw9,fpga1,fpga2,fpga3} for handling a large volume of elements, though they have individual drawbacks.
The {\it Systolic Arrays} and {\it Shift Registers} based approaches \cite{hw8,hw9}, for example, are not scalable and require $O(n)$ comparators for $n$ nodes.
FPGA-based pipelined heap, presented by Ioannou {\it et. al} \cite{fpga1}, is scalable and can run for 64K nodes, but it stalls parallel insert-delete operations. Calendar queues \cite{hw1} incur significant hardware cost when supporting a large priority set. A hybrid priority queue architecture proposed by \cite{hwsw1} operates on $O(log \ n)$ time.

All of the existing works on hardware-based parallel priority queues, however, share a common problem: they do not address {\it holes} created as a result of insert operations in parallel with delete operations. 
Holes occupy storage elements but do not have valid data. 
Retaining them not only introduces additional overhead, but it also leads to an unbalanced tree, which may result in longer response time.
Although holes can be eliminated by rebalancing the tree, doing so requires a non-negligible number of cycles, during which no further insert or delete operations can be processed.

Toward this end, this paper proposes an efficient, parallel hardware-based binary heap implementation. It also demonstrates the negative effect of hole with respect to hardware cost and response time and shows how holes can be minimized. 
The contributions of this paper are summarized as follows:
\begin{itemize}
\item {\bf Hole minimization:} We demonstrate the significant consequences of holes in terms of hardware and response time. The proposed approach minimizes holes created by parallel operations on a priority queue. Our hole minimization technique reduces hardware cost by 37.76\% in terms of number of lookup tables (LUTs) and average response time by 14.48\%.
\item {\bf Hardware sharing:} Sharing hardware between two consecutive pipelined levels reduces the hardware cost. The hardware sharing technique contributes a 7.70\% cost reduction.
\item {\bf Replacement operation:} Upon detection of a min-delete operation immediately followed by an insert operation, a {\it replacement} operation substitutes these two operations. This method reduces average response time by 30.36\%.
\end{itemize}

The rest of this paper is organized as follows:
\autoref{s:background} contains an overview of literature related to this work and discusses holes in parallel implementations in more details.
\autoref{s:impl} presents our proposed design with hole minimization. 
We also propose several optimization techniques for performance efficiency and hardware minimization.
\autoref{s:eval} describes the implementation results along with performance comparisons with existing designs.
\autoref{s:cons} concludes the paper and identifies some directions for future research.
